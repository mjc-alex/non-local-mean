## 第一章 CUDA的异构并行计算
### 并行计算
- 注意分析数据相关性，决定串行与并行
- 应用程序中有两种基本的并行类型
	- 数据并行
		当可以同时处理许多数据时，这就是数据并行。数据并行的重点在于利用多核系统对数据进行分配。
	- 任务并行
		当许多任务或函数可以独立地、大规模地并行执行时，这就是任务并行。任务并行的重点在于利用多核系统对任务进行分配。 
	本书的重点便是如何使用CUDA编程解决数据并行问题
- 数据并行程序设计
	- 第一步是把数据依据线程进行划分，块划分（block partitioning）和周期划分 （cyclic partitioning）。如何在线程中分配数据不仅与数据的物理储存方式密切相关，并且与每个线程的执行次序也有很大关系。组织线程的方式对程序的性能有很大的影响
### 异构计算
- ![](images/Pasted%20image%2020240107204757.png)
- 峰值计算性能是用来评估计算容量的一个指标，通常定义为每秒能处理的单精度或双精度浮点运算的数量。峰值性能通常用GFlops（每秒十亿次浮点运算）或TFlops（每秒万 亿次浮点运算）来表示。内存带宽是从内存中读取或写入数据的比率。内存带宽通常用 GB/s表示。
- 为了支持使用CPU＋GPU异构系统架构来执行应用程序，NVIDIA设计了 一个被称为CUDA(Compute Unified Device Architecture)的编程模型。
- GPU上的线程是高度轻量级的。在一个典型的系统中会有成千上万的线程排队等待工作。如果GPU必须等待一组线程执行结束，那么它只要调用另一组线程执行其他任务即可。GPU的核是用来处理大量并发的、轻量级的线程，以最大限度地提高吞吐量。
- 一个CUDA程序包含了以下两个部分的混合：在CPU上运行的主机代码，在GPU上运行的设备代码。
	![](images/Pasted%20image%2020240107210337.png)
## 第二章 CUDA编程模型
### 概述
- 一个典型的CUDA程序实现流程遵循以下模式。 
1. 把数据从CPU内存拷贝到GPU内存。 
2. 调用核函数对存储在GPU内存中的数据进行操作。 
3. 将数据从GPU内存传送回到CPU内存。
- CUDA编程模型假设系统是由一个主机和一个设备组成的，而且各自拥有独立的内 存。核函数是在设备上运行的。
### 内存管理
- 在GPU内存层次结构中，最主要的两种内存是全局内存和共享内存。全局类似于CPU 的系统内存，而共享内存类似于CPU的缓存。然而GPU的共享内存可以由CUDA C的内核 直接控制。
- 当数据被转移到GPU的全局内存后，主机端调用核函数在GPU上进行数组求和。一旦 内核被调用，控制权立刻被传回主机，这样的话，当核函数在GPU上运行时，主机可以执行其他函数。因此，内核与主机是异步的。
### 线程管理
![](images/Pasted%20image%2020240110153626.png)
由一个内核启动所产生的所有线程统称为一个网格。**同一网格中的所有线程共享相同的全局内存空间**。一个网格由多个线程块构成,相同块内的线程可以通过：同步、共享内存进行协作，不同线程块内的线程不能协作。
- 线程依靠以下两个坐标变量来区分彼此。 **blockIdx**（线程块在线程格内的索引） **threadIdx**（块内的线程索引） 这些变量是核函数中需要预初始化的内置变量。坐标变量是基于uint3定义的CUDA内置的向量类型，是一个包含3个无符号整数的 结构，可以通过x、y、z三个字段来指定。
- CUDA可以组织三维的网格和块。网格和块的维度由下列两个内置变量指定。 **blockDim**（线程块的维度，用每个线程块中的线程数来表示） **gridDim**（线程格的维度，用每个线程格中的线程数来表示） 它们是dim3类型的变量，是基于uint3定义的整数型向量，用来表示维度。当定义一个 dim3类型的变量时，所有未指定的元素都被初始化为1。dim3类型变量中的每个组件可以 通过它的x、y、z字段获得。
- 通常，一个线程格会被组织成线程块的二维数组形式，一个线程块会被组织成线程的 三维数组形式。
- 在CUDA程序中有两组不同的网格和块变量：手动定义的dim3数据类型和预定义的 uint3数据类型。在主机端，作为内核调用的一部分，你可以使用dim3数据类型定义一个网格和块的维度。当执行核函数时，CUDA运行时会生成相应的内置预初始化的网格、块和线程变量，它们在核函数内均可被访问到且为unit3类型。手动定义的dim3类型的网格和块变量仅在主机端可见，而unit3类型的内置预初始化的网格和块变量仅在设备端可见。
```c
// define total data element
int nElem = 6;
// define grid and block structure
dim3 block(3);
dim3 grid((nElem + block.x - 1) / block.x);
output:
grid.x 2 grid.y 1 grid.z 1                     
block.x 3 block.y 1 block.z 1
```
### CUDA C扩展
#### 函数限定符
__device__: 声明在设备上的函数，无法被CPU调用，只能由GPU端程序调用，只能被__global__或__device__函数调用
__global__: 声明的函数称为kernel函数。该函数只能由CPU端调用，执行在GPU上，Kernel函数类型必须是void,返回类型必须为空
__host__： 声明在主机上执行的函数，仅在CPU端调用，一般可省略，只有该函数同时存在被设备和主机调用的情况才需要添加。且无法与__global__联用。
#### 变量限定符
__device__: 位于设备端全局内存中
__constant__: 全局不可修改的变量，只能在设备端使用
__shared__: 声明在共享存储中的变量，仅供block中所有thread共享访存，退出kernel后失效。
#### 核函数
`kernel_name <<<grid, block>>(argument)`
## CUDA执行模型
### 概述

#### thread
在GPU编程中，线程是最基本的执行单元。一个GPU程序会同时启动成千上万个线程来执行任务
#### warp
GPU 的每一行由1个控制单元加上若干计算单元所组成，这些所有的计算单元执行的控制指令是一个。这其实就是个非常典型的单指令多线程机制（SIMT）。
单指令多线程机制是说：多个线程同时执行相同的指令序列，但是每个线程可以处理不同的数据。这些线程通常被分组成更小的线程块，每个线程块中的线程可以协调执行相同的指令。
warp是硬件级别上的调度单位，**一个 warp 包含32个并行 thread**，这些 thread 以不同数据资源执行相同的指令。
#### Block
- 块是线程的集合，它们被组织成一个工作单元，可以共享内存和同步。
- 在GPU编程中，通常会将线程划分成若干个块，以便更有效地管理和协调线程的执行。
- 块内的线程可以进行协作和通信，通常通过共享内存来提高性能。
- Block 间并行执行，并且无法通信，也没有执行顺序
#### Grid
多个block则会再构成grid。kernel 在 device 上执行时，实际上是启动很多线程，一个kernel 所启动的所有线程称为一个网格(grid)。同一个网格上的线程共享相同的全局内存空间。
当一个 kernel 被执行时，grid 中的线程块被分配到 SM (多核处理器) 上，**一个线程块的 thread 只能在一个SM 上调度，SM 一般可以调度多个线程块**，大量的 thread 可能被分到不同的 SM 上。每个 thread 拥有它自己的程序计数器和状态寄存器，并且用该线程自己的数据执行指令，这就是所谓的 Single Instruction Multiple Thread (SIMT)

每个 thread 都有自己的一份 register 和 local memory 的空间。同一个 block 中的每个 thread 则有共享的一份 share memory。此外，所有的 thread (包括不同 block 的 thread) 都共享一份 global memory。不同的 grid 则有各自的 global memory。
![](images/Pasted%20image%2020240228163618.png)
#### 缓存机制
- GPU 内存硬件的分类， (on chip) 内存和片下 (off chip) 内存。  
	**片上内存**主要用于**缓存 (cache)** 以及少量特殊存储单元（如texture）。特点是速度快，存储空间小；  
	**片下内存**主要用于全局存储 (global memory) **即常说的显存**，特点是速度相对慢，存储空间大。不同于 CPU 系统内存可扩展的设计，GPU 的整体内存大小是固定的，在选择好显卡型号后就会确定好，包括缓存和全局存储。
	- L1/L2 cache：多级缓存，位置在 GPU 芯片内部；
	- GPU DRAM：通常所指的显存；
	在磁盘/硬盘（Disk/SSD）上面的数据传入到 GPU 的内存要经过：**硬盘 -> 系统内存 -> GPU 内存**的过程。这个速度非常慢，要极力避免这种传输。
- 按照存储功能进行细分，GPU 内存可以分为：局部内存（local memory）、全局内存（global memory）、常量内存（constant memory）、共享内存（shared memory）、寄存器（register）、L1/L2 缓存等。
	其中全局内存、局部内存、常量内存都是片下内存，储存在 HBM （显存）上。所以我们说 HBM 的**大部分**作为全局内存。
- 共享内存（shared memory) 是一种在线程块内能访问的内存，是片上（on chip）存储，访问速度较快。
	共享内存主要是缓存一些需要反复读写的数据。
	注：共享内存与 L1 缓存的位置、速度极其类似，区别在于共享内存的控制与生命周期管理与 L1 不同：共享内存受用户控制，L1 受系统控制。共享内存更利于线程块之间数据交互。
- 全局内存（global memory）能被 GPU 的所有线程访问，全局共享。它是片下（off chip）内存，前面提到的硬件 HBM 中的大部分都是用作全局内存。跟 CPU 架构一样，运算单元不能直接使用全局内存的数据，需要经过缓存
- 局部内存 (local memory) 是线程独享的内存资源，线程之间不可以相互访问。局部内存属于片下内存，所以访问速度跟全局内存一样。它主要是用来应对**寄存器不足**时的场景，即在线程申请的变量超过可用的寄存器大小时，nvcc 会自动将一部数据放置到片下内存里。
- 常量内存（constant memory）是片下（off chip）存储，但是通过特殊的常量内存缓存（constant cache）进行缓存读取，它是只读内存。

	常量内存主要是解决一个 warp scheduler 内多个线程**访问相同数据**时速度太慢的问题。假设所有线程都需要访问一个 constant_A 的常量，在存储介质上 constant_A 的数据只保存了一份，而内存的物理读取方式决定了多个线程不能在同一时刻读取到该变量，所以会出现先后访问的问题，这样使得并行计算的线程出现了运算时差。常量内存正是解决这样的问题而设置的，它有对应的 cache 位置产生多个副本，让线程访问时不存在冲突，从而保证并行度。
- SM：从 G80 提出的概念，中文称流式多处理器，核心组件包括CUDA核心、共享内存、寄存器等。SM包含许多为线程执行数学运算的Core，是 NVIDA 的核心。

	在CUDA中，可以并发地执行数百个线程。一个 block 上线程是放在同一个 SM，一个 SM 的有限 Cache 制约了每个 block 的线程数量。
### warp 
#### 分化
线程束分化会导致性能明显地下降。条件分支越多，并行性削弱越严重。 注意，线程束分化只发生在同一个线程束中。在不同的线程束中，不同的条件值不会引起线程束分化。
- 当一个分化的线程采取不同的代码路径时，会产生线程束分化 
- 不同的if-then-else分支会连续执行 
- 尝试调整分支粒度以适应线程束大小的倍数，避免线程束分化
- `if else` is better than `if... if...`
#### 资源分配
线程束的本地执行上下文主要由以下资源组成：·程序计数器 ·寄存器 ·共享内存
当计算资源（如寄存器和共享内存）已分配给线程块时，线程块被称为活跃的块。它所包含的线程束被称为活跃的线程束。
一个SM上的线程束调度器在每个周期都选择活跃的线程束，然后把它们调度到执行 单元。活跃执行的线程束被称为选定的线程束。如果一个活跃的线程束准备执行但尚未执 行，它是一个符合条件的线程束。如果一个线程束没有做好执行的准备，它是一个阻塞的 线程束。如果同时满足以下两个条件则线程束符合执行条件。 ·32个CUDA核心可用于执行 ·当前指令中所有的参数都已就绪

